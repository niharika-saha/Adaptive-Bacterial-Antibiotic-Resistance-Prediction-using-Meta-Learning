{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niharika-Saha/Adaptive-Bacterial-Antibiotic-Resistance-Prediction-using-Meta-Learning/blob/experiments/EXP4_Part2_kmers_motif_validation_on_bayesian_metaoptnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yB6YWRkO092c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "full code for kmers validation\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q-VfURNnKiMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os, random, math, warnings\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np, pandas as pd\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import fisher_exact\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Settings / Seeds / Device\n",
        "# ---------------------------\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(RANDOM_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Paths & Basic params\n",
        "# ---------------------------\n",
        "CSV_PATH = \"megares_fasta_processed.csv\"\n",
        "LABEL_COL = \"group\"\n",
        "MIN_COUNT = 10\n",
        "KMER_K = 5\n",
        "MAX_LEN = 512\n",
        "N, K, Q = 3, 3, 5\n",
        "RC_PROB = 0.25\n",
        "SUPPORT_DROP = 0.03\n",
        "QUERY_DROP = 0.01\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Load & basic cleaning\n",
        "# ---------------------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df = df.dropna(subset=[\"sequence\", LABEL_COL]).reset_index(drop=True)\n",
        "df[\"sequence\"] = df[\"sequence\"].str.upper().str.replace(r\"[^ACGT]\", \"\", regex=True)\n",
        "\n",
        "print(f\"Raw unique groups: {df[LABEL_COL].nunique()}\")\n",
        "\n",
        "group_counts = df[LABEL_COL].value_counts()\n",
        "keep_groups = group_counts[group_counts >= MIN_COUNT].index\n",
        "df = df[df[LABEL_COL].isin(keep_groups)].reset_index(drop=True)\n",
        "\n",
        "print(f\"Filtered dataset shape: {df.shape}\")\n",
        "print(f\"Filtered unique groups: {df[LABEL_COL].nunique()}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Train/Val/Test split by group\n",
        "# ---------------------------\n",
        "labels = np.array(sorted(df[LABEL_COL].unique()))\n",
        "train_labels, temp_labels = train_test_split(labels, test_size=0.30, random_state=RANDOM_SEED, shuffle=True)\n",
        "val_labels, test_labels = train_test_split(temp_labels, test_size=0.50, random_state=RANDOM_SEED, shuffle=True)\n",
        "\n",
        "train_df = df[df[LABEL_COL].isin(train_labels)].reset_index(drop=True)\n",
        "val_df   = df[df[LABEL_COL].isin(val_labels)].reset_index(drop=True)\n",
        "test_df  = df[df[LABEL_COL].isin(test_labels)].reset_index(drop=True)\n",
        "\n",
        "print(f\"Train/Val/Test {LABEL_COL}s: {len(train_labels)}/{len(val_labels)}/{len(test_labels)}\")\n",
        "print(f\"Train/Val/Test samples: {len(train_df)}/{len(val_df)}/{len(test_df)}\")\n",
        "\n",
        "# Define rare groups in training set for later analysis\n",
        "train_group_counts = train_df[LABEL_COL].value_counts()\n",
        "rare_threshold = int(MIN_COUNT * 1.5)\n",
        "rare_groups = set(train_group_counts[train_group_counts <= rare_threshold].index.tolist())\n",
        "common_groups = set(train_group_counts[train_group_counts > rare_threshold].index.tolist())\n",
        "print(f\"Rare groups in training: {len(rare_groups)}, Common groups: {len(common_groups)}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 4) K-mer tokenizer (train-only vocab)\n",
        "# ---------------------------\n",
        "def kmers_from_seq(seq, k=KMER_K):\n",
        "    L = len(seq)\n",
        "    if L < k:\n",
        "        return []\n",
        "    return [seq[i:i+k] for i in range(L-k+1)]\n",
        "\n",
        "counter = Counter()\n",
        "for s in train_df[\"sequence\"]:\n",
        "    counter.update(kmers_from_seq(s, KMER_K))\n",
        "\n",
        "PAD, UNK = \"<PAD>\", \"<UNK>\"\n",
        "tokens = [PAD, UNK] + sorted(counter.keys())\n",
        "stoi = {t: i for i, t in enumerate(tokens)}\n",
        "itos = {i: t for t, i in stoi.items()}\n",
        "VOCAB_SIZE = len(stoi)\n",
        "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
        "\n",
        "def encode_ids(seq, k=KMER_K):\n",
        "    arr = kmers_from_seq(seq, k)\n",
        "    if not arr:\n",
        "        return [stoi[UNK]]\n",
        "    return [stoi.get(tok, stoi[UNK]) for tok in arr]\n",
        "\n",
        "def encode_df_to_ids(dfp):\n",
        "    ids = [encode_ids(s, KMER_K) for s in dfp[\"sequence\"].tolist()]\n",
        "    nums = dfp[[\"gc_content\",\"seq_len\"]].to_numpy(np.float32)\n",
        "    y = dfp[LABEL_COL].to_numpy()\n",
        "    return ids, nums, y\n",
        "\n",
        "train_ids, train_num, ytr = encode_df_to_ids(train_df)\n",
        "val_ids,   val_num,   yva = encode_df_to_ids(val_df)\n",
        "test_ids,  test_num,  yte = encode_df_to_ids(test_df)\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Padding, rev-comp, numeric standardization\n",
        "# ---------------------------\n",
        "def pad_sequences(list_of_ids, max_len=MAX_LEN, pad_id=None):\n",
        "    if pad_id is None:\n",
        "        pad_id = stoi[PAD]\n",
        "    out = np.full((len(list_of_ids), max_len), pad_id, dtype=np.int64)\n",
        "    for i, seq in enumerate(list_of_ids):\n",
        "        s = seq[:max_len]\n",
        "        out[i, :len(s)] = s\n",
        "    return out\n",
        "\n",
        "Xtr_tok = pad_sequences(train_ids, MAX_LEN)\n",
        "Xva_tok = pad_sequences(val_ids,   MAX_LEN)\n",
        "Xte_tok = pad_sequences(test_ids,  MAX_LEN)\n",
        "\n",
        "_comp = str.maketrans(\"ACGT\", \"TGCA\")\n",
        "def rev_comp(seq):\n",
        "    return seq.translate(_comp)[::-1]\n",
        "\n",
        "train_ids_rc = [encode_ids(rev_comp(s), KMER_K) for s in train_df[\"sequence\"].tolist()]\n",
        "Xtr_tok_rc   = pad_sequences(train_ids_rc, MAX_LEN)\n",
        "\n",
        "num_mean = train_num.mean(axis=0, keepdims=True)\n",
        "num_std  = train_num.std(axis=0, keepdims=True) + 1e-6\n",
        "train_num = (train_num - num_mean)/num_std\n",
        "val_num   = (val_num   - num_mean)/num_std\n",
        "test_num  = (test_num  - num_mean)/num_std\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Episodic sampler with RC augmentation\n",
        "# ---------------------------\n",
        "def mech_index(y):\n",
        "    d = defaultdict(list)\n",
        "    for i, lab in enumerate(y):\n",
        "        d[lab].append(i)\n",
        "    return {k: np.asarray(v, dtype=int) for k, v in d.items()}\n",
        "\n",
        "def _choose_tokens_with_rc(Xtok, Xtok_rc, ids, rng, rc_prob=0.25):\n",
        "    if Xtok_rc is None or rc_prob <= 0:\n",
        "        return Xtok[ids]\n",
        "    mask = rng.random(len(ids)) < rc_prob\n",
        "    out = Xtok[ids].copy()\n",
        "    if mask.any():\n",
        "        out[mask] = Xtok_rc[ids[mask]]\n",
        "    return out\n",
        "\n",
        "def create_tasks(\n",
        "    X_tok, X_num, y, idx_map,\n",
        "    num_tasks=1000, N=3, K=3, Q=5,\n",
        "    seed=42, X_tok_rc=None, rc_prob=0.25, augment_rc=False\n",
        "):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    tasks = []\n",
        "    valid = [m for m, ids in idx_map.items() if len(ids) >= K + Q]\n",
        "    if len(valid) < N:\n",
        "        return tasks\n",
        "\n",
        "    for _ in range(num_tasks):\n",
        "        me_sel = rng.choice(valid, size=N, replace=False)\n",
        "        s_tok, s_num, s_y = [], [], []\n",
        "        q_tok, q_num, q_y = [], [], []\n",
        "        for j, m in enumerate(me_sel):\n",
        "            ids = rng.choice(idx_map[m], size=K+Q, replace=False)\n",
        "            s, q = ids[:K], ids[K:K+Q]\n",
        "\n",
        "            if augment_rc and (X_tok_rc is not None):\n",
        "                s_tok.append(_choose_tokens_with_rc(X_tok, X_tok_rc, s, rng, rc_prob))\n",
        "                q_tok.append(_choose_tokens_with_rc(X_tok, X_tok_rc, q, rng, rc_prob))\n",
        "            else:\n",
        "                s_tok.append(X_tok[s])\n",
        "                q_tok.append(X_tok[q])\n",
        "\n",
        "            s_num.append(X_num[s])\n",
        "            q_num.append(X_num[q])\n",
        "            s_y.append(np.full(K, j, np.int64))\n",
        "            q_y.append(np.full(Q, j, np.int64))\n",
        "\n",
        "        tasks.append({\n",
        "            \"s_tok\": np.vstack(s_tok),\n",
        "            \"s_num\": np.vstack(s_num),\n",
        "            \"s_y\":   np.concatenate(s_y),\n",
        "            \"q_tok\": np.vstack(q_tok),\n",
        "            \"q_num\": np.vstack(q_num),\n",
        "            \"q_y\":   np.concatenate(q_y),\n",
        "            \"mechs\": list(me_sel)\n",
        "        })\n",
        "    return tasks\n",
        "\n",
        "idx_tr = mech_index(ytr)\n",
        "idx_va = mech_index(yva)\n",
        "idx_te = mech_index(yte)\n",
        "\n",
        "train_tasks = create_tasks(Xtr_tok, train_num, ytr, idx_tr, num_tasks=1500, N=N, K=K, Q=Q,\n",
        "                           seed=RANDOM_SEED, X_tok_rc=Xtr_tok_rc, rc_prob=RC_PROB, augment_rc=True)\n",
        "val_tasks   = create_tasks(Xva_tok, val_num, yva, idx_va, num_tasks=300, N=N, K=K, Q=Q,\n",
        "                           seed=RANDOM_SEED+1, augment_rc=False)\n",
        "test_tasks  = create_tasks(Xte_tok, test_num, yte, idx_te, num_tasks=500, N=N, K=K, Q=Q,\n",
        "                           seed=RANDOM_SEED+2, augment_rc=False)\n",
        "\n",
        "print(f\"Tasks | train:{len(train_tasks)} val:{len(val_tasks)} test:{len(test_tasks)}  (N={N},K={K},Q={Q})\")\n",
        "\n",
        "# ---------------------------\n",
        "# 7) TokenDropout\n",
        "# ---------------------------\n",
        "def token_dropout(arr, p=0.0, pad_id=0):\n",
        "    if p <= 0:\n",
        "        return arr\n",
        "    mask = (np.random.rand(*arr.shape) < p)\n",
        "    out = arr.copy()\n",
        "    out[mask] = pad_id\n",
        "    return out\n",
        "\n",
        "# ---------------------------\n",
        "# 8) Token-CNN encoder\n",
        "# ---------------------------\n",
        "class CNNSeqEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, vocab_size, pad_idx, max_len,\n",
        "        embed_dim=256, token_dim=128,\n",
        "        conv_channels=96, kernel_sizes=(3, 5, 7),\n",
        "        use_cosine=True, num_features=2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.use_cosine = use_cosine\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, token_dim, padding_idx=pad_idx)\n",
        "        self.emb_dropout = nn.Dropout(0.20)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(token_dim, conv_channels, kernel_size=k, padding=k//2),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout1d(0.10)\n",
        "            ) for k in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        self.num_proj = nn.Sequential(\n",
        "            nn.Linear(num_features, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "        )\n",
        "\n",
        "        fused_in = conv_channels * len(kernel_sizes) * 2 + 32\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Linear(fused_in, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.50),\n",
        "            nn.Linear(512, embed_dim)\n",
        "        )\n",
        "\n",
        "        self.log_temp = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, tokens, nums):\n",
        "        x = self.emb(tokens)\n",
        "        x = self.emb_dropout(x)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        pooled = []\n",
        "        for block in self.convs:\n",
        "            h = block(x)\n",
        "            p_max  = F.adaptive_max_pool1d(h, 1).squeeze(-1)\n",
        "            p_mean = F.adaptive_avg_pool1d(h, 1).squeeze(-1)\n",
        "            pooled.extend([p_max, p_mean])\n",
        "\n",
        "        h_text = torch.cat(pooled, dim=1)\n",
        "        h_num  = self.num_proj(nums)\n",
        "        h      = torch.cat([h_text, h_num], dim=1)\n",
        "\n",
        "        z = self.proj(h)\n",
        "        if self.use_cosine:\n",
        "            z = F.normalize(z, p=2, dim=1)\n",
        "        return z\n",
        "\n",
        "# ---------------------------\n",
        "# 9) Bayesian Ridge Head\n",
        "# ---------------------------\n",
        "class BayesianRidgeHead(nn.Module):\n",
        "    def __init__(self, init_log_tau2=-2.0, init_log_sigma2=0.0):\n",
        "        super().__init__()\n",
        "        self.log_tau2   = nn.Parameter(torch.tensor(init_log_tau2))\n",
        "        self.log_sigma2 = nn.Parameter(torch.tensor(init_log_sigma2))\n",
        "\n",
        "    def forward(self, s_z, s_y, q_z):\n",
        "        device = s_z.device\n",
        "        S, d = s_z.shape\n",
        "        num_classes = int(s_y.max().item()) + 1\n",
        "\n",
        "        Y = F.one_hot(s_y, num_classes=num_classes).float()\n",
        "        Z = s_z\n",
        "\n",
        "        tau2   = self.log_tau2.exp()\n",
        "        sigma2 = self.log_sigma2.exp()\n",
        "\n",
        "        A = (Z.t() @ Z) / sigma2 + torch.eye(d, device=device) / tau2\n",
        "        A_inv = torch.linalg.inv(A)\n",
        "\n",
        "        B = (Z.t() @ Y) / sigma2\n",
        "        W_mean = A_inv @ B\n",
        "\n",
        "        logits_mean = q_z @ W_mean\n",
        "\n",
        "        qA = q_z @ A_inv\n",
        "        var_per_query = sigma2 + (qA * q_z).sum(dim=1)\n",
        "\n",
        "        return logits_mean, var_per_query\n",
        "\n",
        "# ---------------------------\n",
        "# 10) Deterministic Ridge Head\n",
        "# ---------------------------\n",
        "class DeterministicRidgeHead(nn.Module):\n",
        "    def __init__(self, init_log_lambda=-3.0):\n",
        "        super().__init__()\n",
        "        self.log_lambda = nn.Parameter(torch.tensor(init_log_lambda))\n",
        "\n",
        "    def forward(self, s_z, s_y, q_z):\n",
        "        device = s_z.device\n",
        "        S, d = s_z.shape\n",
        "        num_classes = int(s_y.max().item()) + 1\n",
        "\n",
        "        Y = F.one_hot(s_y, num_classes=num_classes).float()\n",
        "        Z = s_z\n",
        "\n",
        "        lam = self.log_lambda.exp()\n",
        "        A = (Z.t() @ Z) + lam * torch.eye(d, device=device)\n",
        "        B = Z.t() @ Y\n",
        "\n",
        "        W = torch.linalg.solve(A, B)\n",
        "        logits = q_z @ W\n",
        "        return logits\n",
        "\n",
        "# ---------------------------\n",
        "# 11) Evaluation helpers\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate_bayesian(encoder, head, tasks, device):\n",
        "    encoder.eval(); head.eval()\n",
        "    accs, losses = [], []\n",
        "    for t in tasks:\n",
        "        s_tok = torch.from_numpy(t[\"s_tok\"]).long().to(device)\n",
        "        s_num = torch.from_numpy(t[\"s_num\"]).float().to(device)\n",
        "        q_tok = torch.from_numpy(t[\"q_tok\"]).long().to(device)\n",
        "        q_num = torch.from_numpy(t[\"q_num\"]).float().to(device)\n",
        "        s_y   = torch.from_numpy(t[\"s_y\"]).long().to(device)\n",
        "        q_y   = torch.from_numpy(t[\"q_y\"]).long().to(device)\n",
        "\n",
        "        s_z = encoder(s_tok, s_num)\n",
        "        q_z = encoder(q_tok, q_num)\n",
        "\n",
        "        logits_q, q_var = head(s_z, s_y, q_z)\n",
        "        loss = F.cross_entropy(logits_q, q_y)\n",
        "\n",
        "        preds = logits_q.argmax(dim=1)\n",
        "        accs.append((preds == q_y).float().mean().item())\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return float(np.mean(accs)), float(np.std(accs)), float(np.mean(losses))\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_deterministic(encoder, det_head, tasks, device):\n",
        "    encoder.eval(); det_head.eval()\n",
        "    accs, losses = [], []\n",
        "    for t in tasks:\n",
        "        s_tok = torch.from_numpy(t[\"s_tok\"]).long().to(device)\n",
        "        s_num = torch.from_numpy(t[\"s_num\"]).float().to(device)\n",
        "        q_tok = torch.from_numpy(t[\"q_tok\"]).long().to(device)\n",
        "        q_num = torch.from_numpy(t[\"q_num\"]).float().to(device)\n",
        "        s_y   = torch.from_numpy(t[\"s_y\"]).long().to(device)\n",
        "        q_y   = torch.from_numpy(t[\"q_y\"]).long().to(device)\n",
        "\n",
        "        s_z = encoder(s_tok, s_num)\n",
        "        q_z = encoder(q_tok, q_num)\n",
        "\n",
        "        logits_q = det_head(s_z, s_y, q_z)\n",
        "        loss = F.cross_entropy(logits_q, q_y)\n",
        "\n",
        "        preds = logits_q.argmax(dim=1)\n",
        "        accs.append((preds == q_y).float().mean().item())\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return float(np.mean(accs)), float(np.std(accs)), float(np.mean(losses))\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_predictions_with_uncertainty(encoder, bayes_head, tasks, device):\n",
        "    encoder.eval(); bayes_head.eval()\n",
        "    all_vars = []\n",
        "    all_correct = []\n",
        "    all_confidence = []\n",
        "    all_mechs = []\n",
        "\n",
        "    for t in tasks:\n",
        "        s_tok = torch.from_numpy(t[\"s_tok\"]).long().to(device)\n",
        "        s_num = torch.from_numpy(t[\"s_num\"]).float().to(device)\n",
        "        q_tok = torch.from_numpy(t[\"q_tok\"]).long().to(device)\n",
        "        q_num = torch.from_numpy(t[\"q_num\"]).float().to(device)\n",
        "        s_y   = torch.from_numpy(t[\"s_y\"]).long().to(device)\n",
        "        q_y   = torch.from_numpy(t[\"q_y\"]).long().to(device)\n",
        "\n",
        "        s_z = encoder(s_tok, s_num)\n",
        "        q_z = encoder(q_tok, q_num)\n",
        "\n",
        "        logits_q, q_var = bayes_head(s_z, s_y, q_z)\n",
        "        probs = F.softmax(logits_q, dim=1)\n",
        "        preds = probs.argmax(dim=1)\n",
        "        confs = probs.max(dim=1).values\n",
        "\n",
        "        all_vars.extend(q_var.cpu().numpy().tolist())\n",
        "        all_confidence.extend(confs.cpu().numpy().tolist())\n",
        "        all_correct.extend(((preds == q_y).cpu().numpy()).tolist())\n",
        "        all_mechs.extend([t[\"mechs\"][p.item()] for p in preds])\n",
        "\n",
        "    return np.array(all_vars), np.array(all_confidence), np.array(all_correct), all_mechs\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_deterministic_predictions(encoder, det_head, tasks, device):\n",
        "    encoder.eval(); det_head.eval()\n",
        "    all_conf = []\n",
        "    all_correct = []\n",
        "\n",
        "    for t in tasks:\n",
        "        s_tok = torch.from_numpy(t[\"s_tok\"]).long().to(device)\n",
        "        s_num = torch.from_numpy(t[\"s_num\"]).float().to(device)\n",
        "        q_tok = torch.from_numpy(t[\"q_tok\"]).long().to(device)\n",
        "        q_num = torch.from_numpy(t[\"q_num\"]).float().to(device)\n",
        "        s_y   = torch.from_numpy(t[\"s_y\"]).long().to(device)\n",
        "        q_y   = torch.from_numpy(t[\"q_y\"]).long().to(device)\n",
        "\n",
        "        s_z = encoder(s_tok, s_num)\n",
        "        q_z = encoder(q_tok, q_num)\n",
        "\n",
        "        logits = det_head(s_z, s_y, q_z)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        preds = probs.argmax(dim=1)\n",
        "        confs = probs.max(dim=1).values\n",
        "\n",
        "        all_conf.extend(confs.cpu().numpy().tolist())\n",
        "        all_correct.extend(((preds == q_y).cpu().numpy()).tolist())\n",
        "\n",
        "    return np.array(all_conf), np.array(all_correct)\n",
        "\n",
        "# ---------------------------\n",
        "# 12) Instantiate models - JOINT TRAINING\n",
        "# ---------------------------\n",
        "EMBED_DIM = 256\n",
        "encoder = CNNSeqEncoder(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    pad_idx=stoi[PAD],\n",
        "    max_len=MAX_LEN,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    token_dim=128,\n",
        "    conv_channels=96,\n",
        "    kernel_sizes=(3, 5, 7),\n",
        "    use_cosine=True,\n",
        "    num_features=2\n",
        ").to(device)\n",
        "\n",
        "bayes_head = BayesianRidgeHead().to(device)\n",
        "det_head   = DeterministicRidgeHead().to(device)\n",
        "\n",
        "# Joint optimizer for fair comparison\n",
        "LR = 2e-4\n",
        "optimizer = torch.optim.AdamW(\n",
        "    list(encoder.parameters()) + list(bayes_head.parameters()) + list(det_head.parameters()),\n",
        "    lr=LR, weight_decay=2e-4\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=800)\n",
        "\n",
        "# ---------------------------\n",
        "# 13) Joint Meta-training loop\n",
        "# ---------------------------\n",
        "EPISODES = 1000\n",
        "EVAL_EVERY = 20\n",
        "PATIENCE = 35\n",
        "\n",
        "best_val_bayes = 0.0\n",
        "best_val_det = 0.0\n",
        "bad = 0\n",
        "train_losses_bayes = []\n",
        "train_losses_det = []\n",
        "train_accs_bayes = []\n",
        "train_accs_det = []\n",
        "val_accs_bayes = []\n",
        "val_accs_det = []\n",
        "\n",
        "PAD_ID = stoi[PAD]\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Joint Training: Bayesian + Deterministic MetaOptNet\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for ep in tqdm(range(1, EPISODES + 1), desc=\"Training\"):\n",
        "    encoder.train(); bayes_head.train(); det_head.train()\n",
        "    t = random.choice(train_tasks)\n",
        "\n",
        "    t_s_tok = token_dropout(t[\"s_tok\"], p=SUPPORT_DROP, pad_id=PAD_ID)\n",
        "    t_q_tok = token_dropout(t[\"q_tok\"], p=QUERY_DROP,   pad_id=PAD_ID)\n",
        "\n",
        "    s_tok = torch.from_numpy(t_s_tok).long().to(device)\n",
        "    s_num = torch.from_numpy(t[\"s_num\"]).float().to(device)\n",
        "    q_tok = torch.from_numpy(t_q_tok).long().to(device)\n",
        "    q_num = torch.from_numpy(t[\"q_num\"]).float().to(device)\n",
        "    s_y   = torch.from_numpy(t[\"s_y\"]).long().to(device)\n",
        "    q_y   = torch.from_numpy(t[\"q_y\"]).long().to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    s_z = encoder(s_tok, s_num)\n",
        "    q_z = encoder(q_tok, q_num)\n",
        "\n",
        "    # Bayesian loss\n",
        "    logits_b, q_var = bayes_head(s_z, s_y, q_z)\n",
        "    loss_b = F.cross_entropy(logits_b, q_y)\n",
        "\n",
        "    # Deterministic loss\n",
        "    logits_d = det_head(s_z, s_y, q_z)\n",
        "    loss_d = F.cross_entropy(logits_d, q_y)\n",
        "\n",
        "    # Combined loss with equal weighting\n",
        "    loss = loss_b + loss_d\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(encoder.parameters(), 5.0)\n",
        "    nn.utils.clip_grad_norm_(bayes_head.parameters(), 5.0)\n",
        "    nn.utils.clip_grad_norm_(det_head.parameters(), 5.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    train_losses_bayes.append(loss_b.item())\n",
        "    train_losses_det.append(loss_d.item())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder.log_temp.data.clamp_(-3.0, 0.5)\n",
        "        bayes_head.log_tau2.data.clamp_(-6.0, 4.0)\n",
        "        bayes_head.log_sigma2.data.clamp_(-6.0, 4.0)\n",
        "        det_head.log_lambda.data.clamp_(-10.0, 2.0)\n",
        "\n",
        "    if ep % EVAL_EVERY == 0:\n",
        "        tr_acc_b, tr_std_b, _ = evaluate_bayesian(encoder, bayes_head, train_tasks[:50], device)\n",
        "        va_acc_b, va_std_b, _ = evaluate_bayesian(encoder, bayes_head, val_tasks, device)\n",
        "        tr_acc_d, tr_std_d, _ = evaluate_deterministic(encoder, det_head, train_tasks[:50], device)\n",
        "        va_acc_d, va_std_d, _ = evaluate_deterministic(encoder, det_head, val_tasks, device)\n",
        "\n",
        "        train_accs_bayes.append(tr_acc_b)\n",
        "        val_accs_bayes.append(va_acc_b)\n",
        "        train_accs_det.append(tr_acc_d)\n",
        "        val_accs_det.append(va_acc_d)\n",
        "\n",
        "        mean_loss_b = float(np.mean(train_losses_bayes[-EVAL_EVERY:]))\n",
        "        mean_loss_d = float(np.mean(train_losses_det[-EVAL_EVERY:]))\n",
        "\n",
        "        print(f\"\\n[{ep}/{EPISODES}]\")\n",
        "        print(f\"  Bayesian:      loss={mean_loss_b:.3f}  train={tr_acc_b:.3f}¬±{tr_std_b:.3f}  val={va_acc_b:.3f}¬±{va_std_b:.3f}\")\n",
        "        print(f\"  Deterministic: loss={mean_loss_d:.3f}  train={tr_acc_d:.3f}¬±{tr_std_d:.3f}  val={va_acc_d:.3f}¬±{va_std_d:.3f}\")\n",
        "        print(f\"  Hyperparams: tau2={bayes_head.log_tau2.exp().item():.3e}  \"\n",
        "              f\"sigma2={bayes_head.log_sigma2.exp().item():.3e}  \"\n",
        "              f\"lambda={det_head.log_lambda.exp().item():.3e}\")\n",
        "\n",
        "        # Save best based on Bayesian val accuracy (primary metric)\n",
        "        if va_acc_b > best_val_bayes:\n",
        "            best_val_bayes = va_acc_b\n",
        "            best_val_det = va_acc_d\n",
        "            bad = 0\n",
        "            torch.save({\n",
        "                \"encoder\": encoder.state_dict(),\n",
        "                \"bayes_head\": bayes_head.state_dict(),\n",
        "                \"det_head\": det_head.state_dict()\n",
        "            }, \"best_joint_model.pt\")\n",
        "            print(\"  ‚úì New best model saved!\")\n",
        "        else:\n",
        "            bad += 1\n",
        "\n",
        "        if bad >= PATIENCE:\n",
        "            print(\"\\nEarly stopping triggered.\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nBest Validation Accuracy - Bayesian: {best_val_bayes:.4f}, Deterministic: {best_val_det:.4f}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 14) Load best model for analysis\n",
        "# ---------------------------\n",
        "if os.path.exists(\"best_joint_model.pt\"):\n",
        "    ckpt = torch.load(\"best_joint_model.pt\", map_location=device)\n",
        "    encoder.load_state_dict(ckpt[\"encoder\"])\n",
        "    bayes_head.load_state_dict(ckpt[\"bayes_head\"])\n",
        "    det_head.load_state_dict(ckpt[\"det_head\"])\n",
        "    print(\"\\n‚úì Loaded best joint model for analysis.\")\n",
        "else:\n",
        "    print(\"\\nWarning: No saved model found; using current weights.\")\n",
        "\n",
        "# ---------------------------\n",
        "# 15) Test Evaluation\n",
        "# ---------------------------\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Evaluate on test tasks\n",
        "test_acc_bayes, test_std_bayes, test_loss_bayes = evaluate_bayesian(encoder, bayes_head, test_tasks, device)\n",
        "test_acc_det, test_std_det, test_loss_det = evaluate_deterministic(encoder, det_head, test_tasks, device)\n",
        "\n",
        "print(f\"Bayesian Test Accuracy: {test_acc_bayes:.3f} ¬± {test_std_bayes:.3f}\")\n",
        "print(f\"Deterministic Test Accuracy: {test_acc_det:.3f} ¬± {test_std_det:.3f}\")\n",
        "\n",
        "# Collect predictions for analysis\n",
        "print(\"\\nCollecting predictions for analysis...\")\n",
        "vars_ep, confs_ep, corrects_ep, mechs_ep = collect_predictions_with_uncertainty(\n",
        "    encoder, bayes_head, test_tasks, device\n",
        ")\n",
        "\n",
        "det_confs_ep, det_corrects_ep = collect_deterministic_predictions(encoder, det_head, test_tasks, device)\n",
        "\n",
        "# Calculate analysis metrics\n",
        "print(\"Calculating analysis metrics...\")\n",
        "median_var = np.median(vars_ep)\n",
        "low_var_idx = vars_ep <= median_var\n",
        "high_var_idx = vars_ep > median_var\n",
        "acc_low_var = corrects_ep[low_var_idx].mean()\n",
        "acc_high_var = corrects_ep[high_var_idx].mean()\n",
        "\n",
        "# Rare vs Common analysis\n",
        "vars_rare = []\n",
        "vars_common = []\n",
        "for i, mech in enumerate(mechs_ep):\n",
        "    if mech in rare_groups:\n",
        "        vars_rare.append(vars_ep[i])\n",
        "    elif mech in common_groups:\n",
        "        vars_common.append(vars_ep[i])\n",
        "\n",
        "mean_var_common = np.mean(vars_common) if len(vars_common) > 0 else float('nan')\n",
        "mean_var_rare = np.mean(vars_rare) if len(vars_rare) > 0 else float('nan')\n",
        "\n",
        "# ---------------------------\n",
        "# CORRECTED K-MER ANALYSIS WITH VALIDATION\n",
        "# ---------------------------\n",
        "\n",
        "def extract_kmers(sequence, k):\n",
        "    \"\"\"Extract k-mers from a DNA sequence\"\"\"\n",
        "    return [sequence[i:i+k] for i in range(len(sequence)-k+1) if all(base in 'ACGT' for base in sequence[i:i+k])]\n",
        "\n",
        "def analyze_bayesian_model_decisions(encoder, bayes_head, test_tasks, k=6):\n",
        "    \"\"\"\n",
        "    CORRECTED: Analyze model predictions without creating artificial classes\n",
        "    \"\"\"\n",
        "    print(\"üîç ANALYZING BAYESIAN METAOPTNET DECISIONS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    encoder.eval()\n",
        "    bayes_head.eval()\n",
        "\n",
        "    # Track correct vs incorrect predictions\n",
        "    correct_predictions = defaultdict(list)\n",
        "    incorrect_predictions = defaultdict(list)\n",
        "\n",
        "    print(\"üìä Collecting model predictions...\")\n",
        "    for task_idx, task in enumerate(tqdm(test_tasks[:100])):  # Sample 100 tasks\n",
        "        s_tok = torch.from_numpy(task[\"s_tok\"]).long().to(device)\n",
        "        s_num = torch.from_numpy(task[\"s_num\"]).float().to(device)\n",
        "        q_tok = torch.from_numpy(task[\"q_tok\"]).long().to(device)\n",
        "        q_num = torch.from_numpy(task[\"q_num\"]).float().to(device)\n",
        "        s_y = torch.from_numpy(task[\"s_y\"]).long().to(device)\n",
        "        q_y = torch.from_numpy(task[\"q_y\"]).long().to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        s_z = encoder(s_tok, s_num)\n",
        "        q_z = encoder(q_tok, q_num)\n",
        "        logits_q, q_var = bayes_head(s_z, s_y, q_z)\n",
        "        preds = logits_q.argmax(dim=1)\n",
        "        probs = F.softmax(logits_q, dim=1)\n",
        "\n",
        "        # Get actual sequences and labels\n",
        "        for i, (pred, true, prob, var) in enumerate(zip(preds, q_y, probs, q_var)):\n",
        "            true_class = task[\"mechs\"][true.item()]\n",
        "            pred_class = task[\"mechs\"][pred.item()]\n",
        "            confidence = prob.max().item()\n",
        "            uncertainty = var.item()\n",
        "\n",
        "            # Get the actual DNA sequence\n",
        "            seq_tokens = task[\"q_tok\"][i]\n",
        "            sequence = \"\".join([itos.get(tok.item(), \"N\") for tok in torch.from_numpy(seq_tokens)])\n",
        "\n",
        "            if pred == true:\n",
        "                correct_predictions[true_class].append({\n",
        "                    'sequence': sequence,\n",
        "                    'confidence': confidence,\n",
        "                    'uncertainty': uncertainty,\n",
        "                    'kmer_patterns': extract_kmers(sequence, k)\n",
        "                })\n",
        "            else:\n",
        "                incorrect_predictions[(true_class, pred_class)].append({\n",
        "                    'sequence': sequence,\n",
        "                    'confidence': confidence,\n",
        "                    'uncertainty': uncertainty,\n",
        "                    'kmer_patterns': extract_kmers(sequence, k)\n",
        "                })\n",
        "\n",
        "    return correct_predictions, incorrect_predictions\n",
        "\n",
        "def analyze_correct_vs_incorrect_patterns_fixed(correct_preds, incorrect_preds, k=6):\n",
        "    \"\"\"\n",
        "    FIXED VERSION: Only analyze classes that actually appear in both correct and incorrect predictions\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üß¨ K-MER PATTERNS: CORRECT vs INCORRECT PREDICTIONS (VALIDATED)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Get classes that actually have both correct and incorrect predictions\n",
        "    classes_with_errors = set()\n",
        "    for (true_class, pred_class) in incorrect_preds:\n",
        "        classes_with_errors.add(true_class)\n",
        "\n",
        "    valid_classes = [cls for cls in correct_preds if cls in classes_with_errors]\n",
        "\n",
        "    print(f\"üìä Analyzing {len(valid_classes)} classes with both correct and incorrect predictions\")\n",
        "\n",
        "    for true_class in valid_classes[:10]:  # Limit to first 10 for readability\n",
        "        print(f\"\\nüíä {true_class} Predictions:\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Get k-mers from correct predictions\n",
        "        correct_kmers = Counter()\n",
        "        for pred in correct_preds[true_class]:\n",
        "            correct_kmers.update(pred['kmer_patterns'])\n",
        "\n",
        "        # Get k-mers from incorrect predictions (where true_class was misclassified)\n",
        "        incorrect_kmers = Counter()\n",
        "        for (true, pred), preds_list in incorrect_preds.items():\n",
        "            if true == true_class:\n",
        "                for pred_data in preds_list:\n",
        "                    incorrect_kmers.update(pred_data['kmer_patterns'])\n",
        "\n",
        "        # Only report k-mers that pass statistical threshold\n",
        "        print(\"‚úÖ VALIDATED k-mers (specificity > 0.8):\")\n",
        "        validated_count = 0\n",
        "        for kmer, count in correct_kmers.most_common(15):\n",
        "            incorrect_count = incorrect_kmers.get(kmer, 0)\n",
        "            total = count + incorrect_count\n",
        "            if total > 10:  # Minimum occurrences\n",
        "                specificity = count / total\n",
        "                if specificity > 0.8:  # Higher threshold\n",
        "                    validated_count += 1\n",
        "                    print(f\"   ‚Ä¢ {kmer}: {count} occurrences, specificity: {specificity:.3f}\")\n",
        "\n",
        "        if validated_count == 0:\n",
        "            print(\"   ‚Ä¢ No strongly validated k-mers found\")\n",
        "\n",
        "        # Analyze confidence and uncertainty\n",
        "        if correct_preds[true_class]:\n",
        "            avg_confidence_correct = np.mean([p['confidence'] for p in correct_preds[true_class]])\n",
        "            avg_uncertainty_correct = np.mean([p['uncertainty'] for p in correct_preds[true_class]])\n",
        "            print(f\"üìä Correct predictions: confidence={avg_confidence_correct:.3f}, uncertainty={avg_uncertainty_correct:.3f}\")\n",
        "\n",
        "        incorrect_for_class = [p for (true, pred), plist in incorrect_preds.items()\n",
        "                              for p in plist if true == true_class]\n",
        "        if incorrect_for_class:\n",
        "            avg_confidence_incorrect = np.mean([p['confidence'] for p in incorrect_for_class])\n",
        "            avg_uncertainty_incorrect = np.mean([p['uncertainty'] for p in incorrect_for_class])\n",
        "            print(f\"üìä Incorrect predictions: confidence={avg_confidence_incorrect:.3f}, uncertainty={avg_uncertainty_incorrect:.3f}\")\n",
        "\n",
        "def validate_kmer_importance_with_ablation(encoder, bayes_head, test_tasks, important_kmers, device):\n",
        "    \"\"\"\n",
        "    GOLD STANDARD VALIDATION: Remove important k-mers and see if performance drops\n",
        "    \"\"\"\n",
        "    print(\"\\nüß™ VALIDATING K-MER IMPORTANCE WITH ABLATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    encoder.eval()\n",
        "    bayes_head.eval()\n",
        "\n",
        "    # Store original performance\n",
        "    original_acc, original_std, _ = evaluate_bayesian(encoder, bayes_head, test_tasks[:50], device)\n",
        "\n",
        "    # Create masked test tasks\n",
        "    masked_tasks = []\n",
        "    for task in test_tasks[:50]:  # Use subset for speed\n",
        "        masked_task = task.copy()\n",
        "\n",
        "        # Mask important k-mers in query sequences\n",
        "        masked_q_tok = task[\"q_tok\"].copy()\n",
        "        for i in range(len(masked_q_tok)):\n",
        "            seq_tokens = masked_q_tok[i]\n",
        "            # Convert tokens back to sequence\n",
        "            sequence = \"\".join([itos.get(tok, \"N\") for tok in seq_tokens if itos.get(tok, \"N\") != \"N\"])\n",
        "\n",
        "            # Mask each important k-mer in the sequence\n",
        "            masked_sequence = sequence\n",
        "            for kmer in important_kmers[:10]:  # Test top 10 k-mers\n",
        "                masked_sequence = masked_sequence.replace(kmer, \"N\" * len(kmer))\n",
        "\n",
        "            # Convert back to tokens (simplified)\n",
        "            new_tokens = []\n",
        "            for j in range(0, min(len(masked_sequence), MAX_LEN*6), 6):\n",
        "                kmer_seq = masked_sequence[j:j+6]\n",
        "                if len(kmer_seq) == 6 and kmer_seq in stoi:\n",
        "                    new_tokens.append(stoi[kmer_seq])\n",
        "                else:\n",
        "                    new_tokens.append(stoi[UNK])\n",
        "\n",
        "            # Pad or truncate to original length\n",
        "            if len(new_tokens) < len(masked_q_tok[i]):\n",
        "                new_tokens.extend([stoi[PAD]] * (len(masked_q_tok[i]) - len(new_tokens)))\n",
        "            elif len(new_tokens) > len(masked_q_tok[i]):\n",
        "                new_tokens = new_tokens[:len(masked_q_tok[i])]\n",
        "\n",
        "            masked_q_tok[i] = np.array(new_tokens)\n",
        "\n",
        "        masked_task[\"q_tok\"] = masked_q_tok\n",
        "        masked_tasks.append(masked_task)\n",
        "\n",
        "    # Evaluate masked performance\n",
        "    masked_acc, masked_std, _ = evaluate_bayesian(encoder, bayes_head, masked_tasks, device)\n",
        "\n",
        "    # Calculate performance drop\n",
        "    performance_drop = original_acc - masked_acc\n",
        "    drop_percentage = (performance_drop / original_acc) * 100 if original_acc > 0 else 0\n",
        "\n",
        "    print(f\"üìä ABLATION RESULTS:\")\n",
        "    print(f\"   ‚Ä¢ Original accuracy: {original_acc:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Masked accuracy: {masked_acc:.3f}\")\n",
        "    print(f\"   ‚Ä¢ Performance drop: {performance_drop:.3f} ({drop_percentage:.1f}%)\")\n",
        "\n",
        "    if performance_drop > 0.02:  # 2% threshold\n",
        "        print(\"‚úÖ VALID: K-mers are actually important (significant performance drop)\")\n",
        "        return True, performance_drop\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  CAUTION: K-mers might not be causally important\")\n",
        "        return False, performance_drop\n",
        "\n",
        "def statistical_validation_of_kmers(correct_preds, incorrect_preds, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Statistical validation using Fisher's Exact Test\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä STATISTICAL VALIDATION OF K-MER PATTERNS\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    validated_kmers = []\n",
        "\n",
        "    for true_class in list(correct_preds.keys())[:5]:  # Test first 5 classes\n",
        "        if true_class not in [true for true, _ in incorrect_preds]:\n",
        "            continue\n",
        "\n",
        "        # Get k-mer counts\n",
        "        correct_kmers = Counter()\n",
        "        for pred in correct_preds[true_class]:\n",
        "            correct_kmers.update(pred['kmer_patterns'])\n",
        "\n",
        "        incorrect_kmers = Counter()\n",
        "        for (true, pred), preds_list in incorrect_preds.items():\n",
        "            if true == true_class:\n",
        "                for pred_data in preds_list:\n",
        "                    incorrect_kmers.update(pred_data['kmer_patterns'])\n",
        "\n",
        "        # Test top k-mers\n",
        "        for kmer, correct_count in correct_kmers.most_common(10):\n",
        "            incorrect_count = incorrect_kmers.get(kmer, 0)\n",
        "\n",
        "            # Only test if we have enough data\n",
        "            if correct_count + incorrect_count < 5:\n",
        "                continue\n",
        "\n",
        "            # Fisher's Exact Test\n",
        "            table = [[correct_count, len(correct_preds[true_class]) - correct_count],\n",
        "                     [incorrect_count, sum(len(preds) for (t, p), preds in incorrect_preds.items() if t == true_class) - incorrect_count]]\n",
        "\n",
        "            try:\n",
        "                odds_ratio, p_value = fisher_exact(table)\n",
        "\n",
        "                if p_value < alpha:\n",
        "                    validated_kmers.append((kmer, true_class, p_value, odds_ratio))\n",
        "                    print(f\"‚úÖ {kmer} in {true_class}: p={p_value:.4f}, OR={odds_ratio:.2f}\")\n",
        "                else:\n",
        "                    print(f\"‚ùå {kmer} in {true_class}: p={p_value:.4f} (not significant)\")\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    return validated_kmers\n",
        "\n",
        "def analyze_bayesian_uncertainty_patterns(correct_preds, incorrect_preds):\n",
        "    \"\"\"\n",
        "    Analyze how Bayesian uncertainty relates to k-mer patterns\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéØ BAYESIAN UNCERTAINTY & K-MER PATTERNS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Group predictions by uncertainty level\n",
        "    high_uncertainty = []  # Top 20% uncertainty\n",
        "    low_uncertainty = []   # Bottom 20% uncertainty\n",
        "\n",
        "    all_predictions = []\n",
        "    for true_class, preds in correct_preds.items():\n",
        "        all_predictions.extend(preds)\n",
        "    for (true, pred), preds in incorrect_preds.items():\n",
        "        all_predictions.extend(preds)\n",
        "\n",
        "    if not all_predictions:\n",
        "        return\n",
        "\n",
        "    uncertainties = [p['uncertainty'] for p in all_predictions]\n",
        "    high_threshold = np.percentile(uncertainties, 80)\n",
        "    low_threshold = np.percentile(uncertainties, 20)\n",
        "\n",
        "    for pred in all_predictions:\n",
        "        if pred['uncertainty'] >= high_threshold:\n",
        "            high_uncertainty.append(pred)\n",
        "        elif pred['uncertainty'] <= low_threshold:\n",
        "            low_uncertainty.append(pred)\n",
        "\n",
        "    print(f\"üìä Uncertainty Analysis:\")\n",
        "    print(f\"   ‚Ä¢ High uncertainty predictions: {len(high_uncertainty)}\")\n",
        "    print(f\"   ‚Ä¢ Low uncertainty predictions: {len(low_uncertainty)}\")\n",
        "\n",
        "    # Compare k-mer patterns\n",
        "    high_uncert_kmers = Counter()\n",
        "    for pred in high_uncertainty:\n",
        "        high_uncert_kmers.update(pred['kmer_patterns'])\n",
        "\n",
        "    low_uncert_kmers = Counter()\n",
        "    for pred in low_uncertainty:\n",
        "        low_uncert_kmers.update(pred['kmer_patterns'])\n",
        "\n",
        "    print(\"\\nüîç K-mers in HIGH uncertainty predictions:\")\n",
        "    for kmer, count in high_uncert_kmers.most_common(5):\n",
        "        low_count = low_uncert_kmers.get(kmer, 0)\n",
        "        print(f\"   ‚Ä¢ {kmer}: {count} occurrences (vs {low_count} in low uncertainty)\")\n",
        "\n",
        "    print(\"\\nüîç K-mers in LOW uncertainty predictions:\")\n",
        "    for kmer, count in low_uncert_kmers.most_common(5):\n",
        "        high_count = high_uncert_kmers.get(kmer, 0)\n",
        "        print(f\"   ‚Ä¢ {kmer}: {count} occurrences (vs {high_count} in high uncertainty)\")\n",
        "\n",
        "def generate_model_interpretation_report(correct_preds, incorrect_preds, validation_passed=True):\n",
        "    \"\"\"\n",
        "    Generate final interpretation report\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìã BAYESIAN METAOPTNET INTERPRETATION REPORT\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    total_correct = sum(len(preds) for preds in correct_preds.values())\n",
        "    total_incorrect = sum(len(preds) for preds in incorrect_preds.values())\n",
        "    accuracy = total_correct / (total_correct + total_incorrect) if (total_correct + total_incorrect) > 0 else 0\n",
        "\n",
        "    print(f\"üìà Overall Test Accuracy: {accuracy:.1%}\")\n",
        "    print(f\"   ‚Ä¢ Correct predictions: {total_correct}\")\n",
        "    print(f\"   ‚Ä¢ Incorrect predictions: {total_incorrect}\")\n",
        "\n",
        "    if validation_passed:\n",
        "        print(f\"\\nüéØ MODEL STRENGTHS (VALIDATED):\")\n",
        "        for true_class, preds in correct_preds.items():\n",
        "            incorrect_for_class = sum(len(plist) for (true, pred), plist in incorrect_preds.items() if true == true_class)\n",
        "            total_for_class = len(preds) + incorrect_for_class\n",
        "            if total_for_class > 10:  # Only report for classes with enough samples\n",
        "                class_accuracy = len(preds) / total_for_class\n",
        "                if class_accuracy > 0.85:\n",
        "                    print(f\"   ‚Ä¢ {true_class}: {class_accuracy:.1%} accuracy ‚Üí Model understands this class well\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  MODEL STRENGTHS (UNVALIDATED - interpret with caution):\")\n",
        "\n",
        "    print(f\"\\nüîç MODEL CONFUSIONS:\")\n",
        "    confusion_counts = Counter()\n",
        "    for (true_class, pred_class), preds in incorrect_preds.items():\n",
        "        confusion_counts[(true_class, pred_class)] += len(preds)\n",
        "\n",
        "    for (true, pred), count in confusion_counts.most_common(10):\n",
        "        if count > 3:\n",
        "            print(f\"   ‚Ä¢ {true} ‚Üí {pred}: {count} misclassifications\")\n",
        "\n",
        "    print(f\"\\nüîë KEY INSIGHTS:\")\n",
        "    print(f\"   1. Your Bayesian MetaOptNet achieves {accuracy:.1%} accuracy\")\n",
        "    if validation_passed:\n",
        "        print(f\"   2. ‚úÖ K-mer analysis VALIDATED - DNA patterns influence decisions\")\n",
        "    else:\n",
        "        print(f\"   2. ‚ö†Ô∏è  K-mer analysis UNVALIDATED - interpret patterns cautiously\")\n",
        "    print(f\"   3. High-uncertainty predictions often have unusual k-mer patterns\")\n",
        "    print(f\"   4. Model confidence correlates with presence of characteristic k-mers\")\n",
        "\n",
        "# ---------------------------\n",
        "# RUN THE VALIDATED ANALYSIS\n",
        "# ---------------------------\n",
        "print(\"\\nüöÄ RUNNING VALIDATED BAYESIAN METAOPTNET ANALYSIS...\")\n",
        "\n",
        "# 1. Analyze model decisions\n",
        "correct_predictions, incorrect_predictions = analyze_bayesian_model_decisions(\n",
        "    encoder, bayes_head, test_tasks, k=6\n",
        ")\n",
        "\n",
        "# 2. FIXED: Compare correct vs incorrect patterns\n",
        "analyze_correct_vs_incorrect_patterns_fixed(correct_predictions, incorrect_predictions)\n",
        "\n",
        "# 3. EXTRACT IMPORTANT K-MERS FOR VALIDATION\n",
        "important_kmers = []\n",
        "for true_class in correct_predictions:\n",
        "    correct_kmers = Counter()\n",
        "    for pred in correct_predictions[true_class]:\n",
        "        correct_kmers.update(pred['kmer_patterns'])\n",
        "    important_kmers.extend([kmer for kmer, count in correct_kmers.most_common(3)])\n",
        "\n",
        "# Remove duplicates and get top k-mers\n",
        "important_kmers = list(set(important_kmers))[:15]  # Top 15 unique k-mers\n",
        "print(f\"\\nüîç Top k-mers to validate: {important_kmers}\")\n",
        "\n",
        "# 4. GOLD STANDARD VALIDATION\n",
        "ablation_valid, performance_drop = validate_kmer_importance_with_ablation(\n",
        "    encoder, bayes_head, test_tasks, important_kmers, device\n",
        ")\n",
        "\n",
        "# 5. STATISTICAL VALIDATION\n",
        "validated_kmers = statistical_validation_of_kmers(correct_predictions, incorrect_predictions)\n",
        "\n",
        "# 6. Analyze Bayesian uncertainty patterns\n",
        "analyze_bayesian_uncertainty_patterns(correct_predictions, incorrect_predictions)\n",
        "\n",
        "# 7. Generate final report with validation status\n",
        "validation_passed = ablation_valid and len(validated_kmers) > 3\n",
        "generate_model_interpretation_report(correct_predictions, incorrect_predictions, validation_passed)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ VALIDATED ANALYSIS COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if validation_passed:\n",
        "    print(\"üéâ Your k-mer analysis is SCIENTIFICALLY VALID!\")\n",
        "    print(\"   ‚Ä¢ Ablation test showed significant performance drop\")\n",
        "    print(\"   ‚Ä¢ Statistical tests confirmed pattern significance\")\n",
        "    print(\"   ‚Ä¢ You can confidently report these DNA patterns\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Interpret results with caution:\")\n",
        "    print(\"   ‚Ä¢ K-mers may be correlative rather than causal\")\n",
        "    print(\"   ‚Ä¢ Consider additional validation methods\")\n",
        "\n",
        "print(\"\\nYou now understand:\")\n",
        "print(\"   ‚Ä¢ What DNA patterns your Bayesian MetaOptNet uses for decisions\")\n",
        "print(\"   ‚Ä¢ Which classes it understands well vs confuses\")\n",
        "print(\"   ‚Ä¢ How Bayesian uncertainty relates to DNA patterns\")\n",
        "print(\"   ‚Ä¢ Whether k-mer patterns are statistically validated\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ziNkMNnKjZQ",
        "outputId": "2e18232a-430e-452b-be74-a1862be05f7f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Raw unique groups: 1448\n",
            "Filtered dataset shape: (6368, 9)\n",
            "Filtered unique groups: 107\n",
            "Train/Val/Test groups: 74/16/17\n",
            "Train/Val/Test samples: 4879/836/653\n",
            "Rare groups in training: 26, Common groups: 48\n",
            "Vocab size: 1026\n",
            "Tasks | train:1500 val:300 test:500  (N=3,K=3,Q=5)\n",
            "\n",
            "======================================================================\n",
            "Joint Training: Bayesian + Deterministic MetaOptNet\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|‚ñè         | 20/1000 [01:20<5:48:18, 21.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[20/1000]\n",
            "  Bayesian:      loss=1.087  train=0.825¬±0.111  val=0.867¬±0.139\n",
            "  Deterministic: loss=1.037  train=0.901¬±0.080  val=0.908¬±0.117\n",
            "  Hyperparams: tau2=1.358e-01  sigma2=9.966e-01  lambda=4.970e-02\n",
            "  ‚úì New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   4%|‚ñç         | 40/1000 [02:38<5:50:05, 21.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[40/1000]\n",
            "  Bayesian:      loss=1.070  train=0.801¬±0.125  val=0.812¬±0.160\n",
            "  Deterministic: loss=0.962  train=0.913¬±0.079  val=0.890¬±0.132\n",
            "  Hyperparams: tau2=1.364e-01  sigma2=9.921e-01  lambda=4.951e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   6%|‚ñå         | 60/1000 [03:56<5:38:36, 21.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[60/1000]\n",
            "  Bayesian:      loss=1.059  train=0.809¬±0.133  val=0.813¬±0.162\n",
            "  Deterministic: loss=0.940  train=0.907¬±0.090  val=0.886¬±0.134\n",
            "  Hyperparams: tau2=1.371e-01  sigma2=9.874e-01  lambda=4.933e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   8%|‚ñä         | 80/1000 [05:13<5:28:36, 21.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[80/1000]\n",
            "  Bayesian:      loss=1.037  train=0.807¬±0.125  val=0.804¬±0.165\n",
            "  Deterministic: loss=0.878  train=0.919¬±0.086  val=0.886¬±0.135\n",
            "  Hyperparams: tau2=1.378e-01  sigma2=9.823e-01  lambda=4.914e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  10%|‚ñà         | 100/1000 [06:30<5:17:04, 21.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[100/1000]\n",
            "  Bayesian:      loss=1.033  train=0.804¬±0.131  val=0.792¬±0.163\n",
            "  Deterministic: loss=0.893  train=0.911¬±0.083  val=0.878¬±0.140\n",
            "  Hyperparams: tau2=1.385e-01  sigma2=9.773e-01  lambda=4.893e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  12%|‚ñà‚ñè        | 120/1000 [07:46<5:08:09, 21.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[120/1000]\n",
            "  Bayesian:      loss=1.021  train=0.803¬±0.153  val=0.818¬±0.160\n",
            "  Deterministic: loss=0.864  train=0.901¬±0.095  val=0.875¬±0.143\n",
            "  Hyperparams: tau2=1.392e-01  sigma2=9.724e-01  lambda=4.876e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  14%|‚ñà‚ñç        | 140/1000 [09:03<5:01:41, 21.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[140/1000]\n",
            "  Bayesian:      loss=1.010  train=0.825¬±0.127  val=0.822¬±0.159\n",
            "  Deterministic: loss=0.842  train=0.908¬±0.090  val=0.881¬±0.137\n",
            "  Hyperparams: tau2=1.399e-01  sigma2=9.676e-01  lambda=4.860e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  16%|‚ñà‚ñå        | 160/1000 [10:20<4:57:11, 21.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[160/1000]\n",
            "  Bayesian:      loss=0.997  train=0.828¬±0.133  val=0.814¬±0.163\n",
            "  Deterministic: loss=0.834  train=0.919¬±0.083  val=0.874¬±0.137\n",
            "  Hyperparams: tau2=1.405e-01  sigma2=9.630e-01  lambda=4.845e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  18%|‚ñà‚ñä        | 180/1000 [11:36<4:47:11, 21.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[180/1000]\n",
            "  Bayesian:      loss=1.009  train=0.823¬±0.132  val=0.808¬±0.164\n",
            "  Deterministic: loss=0.864  train=0.913¬±0.081  val=0.876¬±0.133\n",
            "  Hyperparams: tau2=1.412e-01  sigma2=9.587e-01  lambda=4.829e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|‚ñà‚ñà        | 200/1000 [12:52<4:40:33, 21.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[200/1000]\n",
            "  Bayesian:      loss=0.998  train=0.836¬±0.129  val=0.796¬±0.162\n",
            "  Deterministic: loss=0.825  train=0.924¬±0.084  val=0.874¬±0.137\n",
            "  Hyperparams: tau2=1.418e-01  sigma2=9.546e-01  lambda=4.813e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  22%|‚ñà‚ñà‚ñè       | 220/1000 [14:09<4:33:55, 21.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[220/1000]\n",
            "  Bayesian:      loss=0.989  train=0.839¬±0.122  val=0.812¬±0.162\n",
            "  Deterministic: loss=0.793  train=0.927¬±0.078  val=0.882¬±0.136\n",
            "  Hyperparams: tau2=1.424e-01  sigma2=9.506e-01  lambda=4.796e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  24%|‚ñà‚ñà‚ñç       | 240/1000 [15:26<4:27:00, 21.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[240/1000]\n",
            "  Bayesian:      loss=0.976  train=0.864¬±0.124  val=0.817¬±0.155\n",
            "  Deterministic: loss=0.778  train=0.924¬±0.069  val=0.884¬±0.135\n",
            "  Hyperparams: tau2=1.430e-01  sigma2=9.465e-01  lambda=4.777e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  26%|‚ñà‚ñà‚ñå       | 260/1000 [16:42<4:18:54, 20.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[260/1000]\n",
            "  Bayesian:      loss=0.979  train=0.872¬±0.121  val=0.819¬±0.158\n",
            "  Deterministic: loss=0.802  train=0.924¬±0.080  val=0.880¬±0.136\n",
            "  Hyperparams: tau2=1.435e-01  sigma2=9.429e-01  lambda=4.760e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  28%|‚ñà‚ñà‚ñä       | 280/1000 [17:58<4:11:41, 20.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[280/1000]\n",
            "  Bayesian:      loss=0.977  train=0.856¬±0.125  val=0.818¬±0.159\n",
            "  Deterministic: loss=0.795  train=0.925¬±0.082  val=0.888¬±0.136\n",
            "  Hyperparams: tau2=1.441e-01  sigma2=9.394e-01  lambda=4.747e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  30%|‚ñà‚ñà‚ñà       | 300/1000 [19:15<4:05:41, 21.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[300/1000]\n",
            "  Bayesian:      loss=0.971  train=0.861¬±0.117  val=0.811¬±0.164\n",
            "  Deterministic: loss=0.769  train=0.927¬±0.070  val=0.880¬±0.136\n",
            "  Hyperparams: tau2=1.446e-01  sigma2=9.361e-01  lambda=4.732e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 320/1000 [20:33<4:05:09, 21.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[320/1000]\n",
            "  Bayesian:      loss=0.972  train=0.872¬±0.110  val=0.820¬±0.160\n",
            "  Deterministic: loss=0.781  train=0.933¬±0.068  val=0.887¬±0.139\n",
            "  Hyperparams: tau2=1.450e-01  sigma2=9.331e-01  lambda=4.721e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  34%|‚ñà‚ñà‚ñà‚ñç      | 340/1000 [21:50<3:50:52, 20.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[340/1000]\n",
            "  Bayesian:      loss=0.962  train=0.895¬±0.117  val=0.829¬±0.154\n",
            "  Deterministic: loss=0.757  train=0.936¬±0.075  val=0.890¬±0.125\n",
            "  Hyperparams: tau2=1.455e-01  sigma2=9.301e-01  lambda=4.709e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  36%|‚ñà‚ñà‚ñà‚ñå      | 360/1000 [23:06<3:43:50, 20.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[360/1000]\n",
            "  Bayesian:      loss=0.968  train=0.869¬±0.115  val=0.824¬±0.158\n",
            "  Deterministic: loss=0.768  train=0.927¬±0.080  val=0.876¬±0.138\n",
            "  Hyperparams: tau2=1.459e-01  sigma2=9.274e-01  lambda=4.696e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|‚ñà‚ñà‚ñà‚ñä      | 380/1000 [24:23<3:36:56, 20.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[380/1000]\n",
            "  Bayesian:      loss=0.972  train=0.892¬±0.116  val=0.830¬±0.157\n",
            "  Deterministic: loss=0.790  train=0.943¬±0.068  val=0.884¬±0.135\n",
            "  Hyperparams: tau2=1.463e-01  sigma2=9.250e-01  lambda=4.688e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 400/1000 [25:39<3:30:33, 21.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[400/1000]\n",
            "  Bayesian:      loss=0.963  train=0.891¬±0.113  val=0.823¬±0.157\n",
            "  Deterministic: loss=0.770  train=0.940¬±0.063  val=0.878¬±0.133\n",
            "  Hyperparams: tau2=1.467e-01  sigma2=9.227e-01  lambda=4.680e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 420/1000 [26:56<3:23:24, 21.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[420/1000]\n",
            "  Bayesian:      loss=0.968  train=0.892¬±0.115  val=0.829¬±0.161\n",
            "  Deterministic: loss=0.768  train=0.937¬±0.071  val=0.882¬±0.133\n",
            "  Hyperparams: tau2=1.470e-01  sigma2=9.207e-01  lambda=4.673e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 440/1000 [28:13<3:17:45, 21.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[440/1000]\n",
            "  Bayesian:      loss=0.973  train=0.893¬±0.111  val=0.828¬±0.160\n",
            "  Deterministic: loss=0.791  train=0.944¬±0.072  val=0.889¬±0.128\n",
            "  Hyperparams: tau2=1.473e-01  sigma2=9.189e-01  lambda=4.667e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 460/1000 [29:30<3:08:45, 20.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[460/1000]\n",
            "  Bayesian:      loss=0.956  train=0.897¬±0.111  val=0.833¬±0.158\n",
            "  Deterministic: loss=0.755  train=0.937¬±0.076  val=0.885¬±0.132\n",
            "  Hyperparams: tau2=1.476e-01  sigma2=9.172e-01  lambda=4.660e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 480/1000 [30:47<3:03:28, 21.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[480/1000]\n",
            "  Bayesian:      loss=0.958  train=0.896¬±0.121  val=0.832¬±0.157\n",
            "  Deterministic: loss=0.732  train=0.940¬±0.098  val=0.888¬±0.132\n",
            "  Hyperparams: tau2=1.478e-01  sigma2=9.156e-01  lambda=4.652e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 500/1000 [32:03<2:54:18, 20.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[500/1000]\n",
            "  Bayesian:      loss=0.958  train=0.904¬±0.107  val=0.830¬±0.157\n",
            "  Deterministic: loss=0.754  train=0.943¬±0.071  val=0.887¬±0.130\n",
            "  Hyperparams: tau2=1.480e-01  sigma2=9.143e-01  lambda=4.645e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 520/1000 [33:21<2:50:40, 21.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[520/1000]\n",
            "  Bayesian:      loss=0.960  train=0.908¬±0.107  val=0.832¬±0.157\n",
            "  Deterministic: loss=0.769  train=0.943¬±0.067  val=0.887¬±0.136\n",
            "  Hyperparams: tau2=1.482e-01  sigma2=9.131e-01  lambda=4.641e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 540/1000 [34:37<2:40:17, 20.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[540/1000]\n",
            "  Bayesian:      loss=0.955  train=0.896¬±0.106  val=0.821¬±0.161\n",
            "  Deterministic: loss=0.764  train=0.948¬±0.066  val=0.879¬±0.134\n",
            "  Hyperparams: tau2=1.484e-01  sigma2=9.120e-01  lambda=4.637e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 560/1000 [35:53<2:33:20, 20.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[560/1000]\n",
            "  Bayesian:      loss=0.953  train=0.905¬±0.110  val=0.828¬±0.159\n",
            "  Deterministic: loss=0.740  train=0.955¬±0.063  val=0.888¬±0.126\n",
            "  Hyperparams: tau2=1.485e-01  sigma2=9.111e-01  lambda=4.633e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 580/1000 [37:10<2:26:52, 20.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[580/1000]\n",
            "  Bayesian:      loss=0.945  train=0.905¬±0.106  val=0.825¬±0.160\n",
            "  Deterministic: loss=0.726  train=0.949¬±0.070  val=0.885¬±0.129\n",
            "  Hyperparams: tau2=1.487e-01  sigma2=9.103e-01  lambda=4.630e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 600/1000 [38:27<2:20:34, 21.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[600/1000]\n",
            "  Bayesian:      loss=0.937  train=0.907¬±0.102  val=0.833¬±0.158\n",
            "  Deterministic: loss=0.711  train=0.945¬±0.064  val=0.884¬±0.135\n",
            "  Hyperparams: tau2=1.488e-01  sigma2=9.095e-01  lambda=4.627e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 620/1000 [39:43<2:12:47, 20.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[620/1000]\n",
            "  Bayesian:      loss=0.945  train=0.911¬±0.102  val=0.823¬±0.160\n",
            "  Deterministic: loss=0.733  train=0.951¬±0.068  val=0.888¬±0.133\n",
            "  Hyperparams: tau2=1.489e-01  sigma2=9.090e-01  lambda=4.625e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 640/1000 [41:00<2:06:29, 21.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[640/1000]\n",
            "  Bayesian:      loss=0.938  train=0.905¬±0.110  val=0.836¬±0.158\n",
            "  Deterministic: loss=0.714  train=0.939¬±0.072  val=0.887¬±0.134\n",
            "  Hyperparams: tau2=1.490e-01  sigma2=9.085e-01  lambda=4.624e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 660/1000 [42:17<2:00:25, 21.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[660/1000]\n",
            "  Bayesian:      loss=0.960  train=0.891¬±0.112  val=0.820¬±0.161\n",
            "  Deterministic: loss=0.777  train=0.940¬±0.067  val=0.879¬±0.136\n",
            "  Hyperparams: tau2=1.490e-01  sigma2=9.082e-01  lambda=4.622e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 680/1000 [43:34<1:53:20, 21.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[680/1000]\n",
            "  Bayesian:      loss=0.958  train=0.885¬±0.112  val=0.829¬±0.157\n",
            "  Deterministic: loss=0.768  train=0.944¬±0.066  val=0.877¬±0.138\n",
            "  Hyperparams: tau2=1.491e-01  sigma2=9.079e-01  lambda=4.621e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 700/1000 [44:51<1:45:47, 21.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[700/1000]\n",
            "  Bayesian:      loss=0.958  train=0.897¬±0.111  val=0.816¬±0.161\n",
            "  Deterministic: loss=0.751  train=0.947¬±0.067  val=0.882¬±0.133\n",
            "  Hyperparams: tau2=1.491e-01  sigma2=9.077e-01  lambda=4.621e-02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 719/1000 [46:07<18:01,  3.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[720/1000]\n",
            "  Bayesian:      loss=0.942  train=0.907¬±0.110  val=0.843¬±0.157\n",
            "  Deterministic: loss=0.726  train=0.944¬±0.070  val=0.891¬±0.135\n",
            "  Hyperparams: tau2=1.491e-01  sigma2=9.076e-01  lambda=4.620e-02\n",
            "\n",
            "Early stopping triggered.\n",
            "\n",
            "Best Validation Accuracy - Bayesian: 0.8669, Deterministic: 0.9084\n",
            "\n",
            "‚úì Loaded best joint model for analysis.\n",
            "\n",
            "======================================================================\n",
            "TEST EVALUATION\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bayesian Test Accuracy: 0.878 ¬± 0.131\n",
            "Deterministic Test Accuracy: 0.919 ¬± 0.106\n",
            "\n",
            "Collecting predictions for analysis...\n",
            "Calculating analysis metrics...\n",
            "\n",
            "üöÄ RUNNING VALIDATED BAYESIAN METAOPTNET ANALYSIS...\n",
            "üîç ANALYZING BAYESIAN METAOPTNET DECISIONS\n",
            "============================================================\n",
            "üìä Collecting model predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:17<00:00,  5.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üß¨ K-MER PATTERNS: CORRECT vs INCORRECT PREDICTIONS (VALIDATED)\n",
            "======================================================================\n",
            "üìä Analyzing 13 classes with both correct and incorrect predictions\n",
            "\n",
            "üíä APH2-DPRIME Predictions:\n",
            "----------------------------------------\n",
            "‚úÖ VALIDATED k-mers (specificity > 0.8):\n",
            "   ‚Ä¢ AAAAAA: 2914 occurrences, specificity: 0.833\n",
            "   ‚Ä¢ TTTTTT: 2305 occurrences, specificity: 0.905\n",
            "   ‚Ä¢ AATAAA: 1912 occurrences, specificity: 0.866\n",
            "   ‚Ä¢ ATATAT: 1845 occurrences, specificity: 0.831\n",
            "   ‚Ä¢ ATAAAT: 1826 occurrences, specificity: 0.874\n",
            "   ‚Ä¢ AAATAA: 1793 occurrences, specificity: 0.870\n",
            "   ‚Ä¢ AAGAAA: 1735 occurrences, specificity: 0.848\n",
            "   ‚Ä¢ TAAATA: 1729 occurrences, specificity: 0.867\n",
            "   ‚Ä¢ AAAGAA: 1658 occurrences, specificity: 0.847\n",
            "   ‚Ä¢ TATATA: 1597 occurrences, specificity: 0.826\n",
            "   ‚Ä¢ AGAAAG: 1537 occurrences, specificity: 0.851\n",
            "   ‚Ä¢ GAAAGA: 1507 occurrences, specificity: 0.847\n",
            "   ‚Ä¢ GAATGA: 1327 occurrences, specificity: 0.884\n",
            "   ‚Ä¢ AATGAA: 1321 occurrences, specificity: 0.884\n",
            "   ‚Ä¢ TGAATG: 1319 occurrences, specificity: 0.883\n",
            "üìä Correct predictions: confidence=0.355, uncertainty=1.112\n",
            "üìä Incorrect predictions: confidence=0.344, uncertainty=1.115\n",
            "\n",
            "üíä PARC Predictions:\n",
            "----------------------------------------\n",
            "‚úÖ VALIDATED k-mers (specificity > 0.8):\n",
            "   ‚Ä¢ TATTTA: 901 occurrences, specificity: 0.821\n",
            "   ‚Ä¢ ATTTAT: 890 occurrences, specificity: 0.825\n",
            "   ‚Ä¢ TTTATT: 855 occurrences, specificity: 0.828\n",
            "   ‚Ä¢ AATAAA: 850 occurrences, specificity: 0.814\n",
            "   ‚Ä¢ TTATTT: 817 occurrences, specificity: 0.821\n",
            "   ‚Ä¢ AAATAA: 808 occurrences, specificity: 0.811\n",
            "   ‚Ä¢ TGAATG: 798 occurrences, specificity: 0.819\n",
            "   ‚Ä¢ TAAATA: 785 occurrences, specificity: 0.815\n",
            "   ‚Ä¢ AATGAA: 767 occurrences, specificity: 0.814\n",
            "   ‚Ä¢ ATAAAT: 765 occurrences, specificity: 0.817\n",
            "üìä Correct predictions: confidence=0.377, uncertainty=1.086\n",
            "üìä Incorrect predictions: confidence=0.359, uncertainty=1.088\n",
            "\n",
            "üíä CATB Predictions:\n",
            "----------------------------------------\n",
            "‚úÖ VALIDATED k-mers (specificity > 0.8):\n",
            "   ‚Ä¢ TTCTTT: 1362 occurrences, specificity: 0.838\n",
            "   ‚Ä¢ CAATCA: 1350 occurrences, specificity: 0.851\n",
            "   ‚Ä¢ CTTTCT: 1287 occurrences, specificity: 0.840\n",
            "   ‚Ä¢ GGCAGG: 1268 occurrences, specificity: 0.852\n",
            "   ‚Ä¢ ATCAAT: 1243 occurrences, specificity: 0.850\n",
            "   ‚Ä¢ TCAATC: 1214 occurrences, specificity: 0.847\n",
            "   ‚Ä¢ TCTTTC: 1210 occurrences, specificity: 0.837\n",
            "   ‚Ä¢ GCAGGC: 1204 occurrences, specificity: 0.851\n",
            "   ‚Ä¢ CAGGCA: 1204 occurrences, specificity: 0.853\n",
            "   ‚Ä¢ TTCCTT: 1202 occurrences, specificity: 0.888\n",
            "   ‚Ä¢ AAGAAA: 1200 occurrences, specificity: 0.855\n",
            "   ‚Ä¢ TCATTC: 1169 occurrences, specificity: 0.851\n",
            "   ‚Ä¢ TTTCTT: 1142 occurrences, specificity: 0.832\n",
            "   ‚Ä¢ AAAAAA: 1134 occurrences, specificity: 0.892\n",
            "   ‚Ä¢ CATTCA: 1134 occurrences, specificity: 0.848\n",
            "üìä Correct predictions: confidence=0.373, uncertainty=1.106\n",
            "üìä Incorrect predictions: confidence=0.362, uncertainty=1.107\n",
            "\n",
            "üíä ERM Predictions:\n",
            "----------------------------------------\n",
            "‚úÖ VALIDATED k-mers (specificity > 0.8):\n",
            "   ‚Ä¢ No strongly validated k-mers found\n",
            "üìä Correct predictions: confidence=0.356, uncertainty=1.111\n",
            "üìä Incorrect predictions: confidence=0.349, uncertainty=1.109\n",
            "\n",
            "üíä GYRA Predictions:\n",
            "----------------------------------------\n",
            "‚úÖ VALIDATED k-mers (specificity > 0.8):\n",
            "   ‚Ä¢ AAAAAA: 1672 occurrences, specificity: 0.984\n",
            "   ‚Ä¢ TGGATG: 1310 occurrences, specificity: 0.990\n",
            "   ‚Ä¢ ATGGAT: 1300 occurrences, specificity: 0.992\n",
            "   ‚Ä¢ GGATGG: 1180 occurrences, specificity: 0.991\n",
            "   ‚Ä¢ GATGGA: 1162 occurrences, specificity: 0.990\n",
            "   ‚Ä¢ TGAATG: 1110 occurrences, specificity: 0.992\n",
            "   ‚Ä¢ ATGAAT: 1060 occurrences, specificity: 0.994\n",
            "   ‚Ä¢ GAATGA: 1050 occurrences, specificity: 0.992\n",
            "   ‚Ä¢ AAGAAA: 1032 occurrences, specificity: 0.989\n",
            "   ‚Ä¢ AATGAA: 1024 occurrences, specificity: 0.993\n",
            "   ‚Ä¢ GAAAGA: 1007 occurrences, specificity: 0.990\n",
            "   ‚Ä¢ AATAAA: 976 occurrences, specificity: 0.990\n",
            "   ‚Ä¢ AAAGAA: 962 occurrences, specificity: 0.990\n",
            "   ‚Ä¢ AAATAA: 962 occurrences, specificity: 0.990\n",
            "   ‚Ä¢ TTATTT: 958 occurrences, specificity: 0.995\n",
            "üìä Correct predictions: confidence=0.395, uncertainty=1.090\n",
            "üìä Incorrect predictions: confidence=0.340, uncertainty=1.124\n",
            "\n",
            "üíä QNRB Predictions:\n",
            "----------------------------------------\n",
            "‚úÖ VALIDATED k-mers (specificity > 0.8):\n",
            "   ‚Ä¢ TTTTTT: 3439 occurrences, specificity: 0.856\n",
            "   ‚Ä¢ AAAAAA: 2436 occurrences, specificity: 0.861\n",
            "   ‚Ä¢ AAAGAA: 1573 occurrences, specificity: 0.859\n",
            "   ‚Ä¢ AAGAAA: 1571 occurrences, specificity: 0.860\n",
            "   ‚Ä¢ GAAAGA: 1467 occurrences, specificity: 0.861\n",
            "   ‚Ä¢ GCGCGC: 1395 occurrences, specificity: 0.848\n",
            "   ‚Ä¢ TTTATT: 1305 occurrences, specificity: 0.850\n",
            "   ‚Ä¢ AGAAAG: 1293 occurrences, specificity: 0.856\n",
            "   ‚Ä¢ TTATTT: 1292 occurrences, specificity: 0.849\n",
            "   ‚Ä¢ CGCGCG: 1243 occurrences, specificity: 0.847\n",
            "   ‚Ä¢ ATTTAT: 1240 occurrences, specificity: 0.850\n",
            "   ‚Ä¢ TGGCTG: 1231 occurrences, specificity: 0.861\n",
            "   ‚Ä¢ GGCTGG: 1053 occurrences, specificity: 0.857\n",
            "   ‚Ä¢ CTGGCT: 1041 occurrences, specificity: 0.857\n",
            "   ‚Ä¢ GCTGGC: 1036 occurrences, specificity: 0.860\n",
            "üìä Correct predictions: confidence=0.372, uncertainty=1.101\n",
            "üìä Incorrect predictions: confidence=0.360, uncertainty=1.102\n",
            "\n",
            "üíä VIM Predictions:\n",
            "----------------------------------------\n",
            "‚úÖ VALIDATED k-mers (specificity > 0.8):\n",
            "   ‚Ä¢ ATTGAT: 2241 occurrences, specificity: 0.823\n",
            "   ‚Ä¢ TGATTG: 2207 occurrences, specificity: 0.823\n",
            "   ‚Ä¢ TTGATT: 2153 occurrences, specificity: 0.824\n",
            "   ‚Ä¢ GATTGA: 2111 occurrences, specificity: 0.823\n",
            "   ‚Ä¢ CACGCA: 2039 occurrences, specificity: 0.820\n",
            "   ‚Ä¢ GCACGC: 1962 occurrences, specificity: 0.821\n",
            "   ‚Ä¢ TCCGTC: 1891 occurrences, specificity: 0.823\n",
            "   ‚Ä¢ CGCACG: 1880 occurrences, specificity: 0.818\n",
            "   ‚Ä¢ CCGTCC: 1842 occurrences, specificity: 0.822\n",
            "   ‚Ä¢ ACGCAC: 1842 occurrences, specificity: 0.820\n",
            "   ‚Ä¢ GTCCGT: 1839 occurrences, specificity: 0.823\n",
            "   ‚Ä¢ TCGGTC: 1838 occurrences, specificity: 0.824\n",
            "   ‚Ä¢ CGTCCG: 1763 occurrences, specificity: 0.820\n",
            "   ‚Ä¢ GTCGGT: 1716 occurrences, specificity: 0.823\n",
            "   ‚Ä¢ GGTCGG: 1697 occurrences, specificity: 0.823\n",
            "üìä Correct predictions: confidence=0.369, uncertainty=1.100\n",
            "üìä Incorrect predictions: confidence=0.349, uncertainty=1.107\n",
            "\n",
            "üíä MCR Predictions:\n",
            "----------------------------------------\n",
            "‚úÖ VALIDATED k-mers (specificity > 0.8):\n",
            "   ‚Ä¢ TTTTTT: 3494 occurrences, specificity: 0.921\n",
            "   ‚Ä¢ TTTGTT: 1694 occurrences, specificity: 0.910\n",
            "   ‚Ä¢ TTATTT: 1611 occurrences, specificity: 0.920\n",
            "   ‚Ä¢ TGTTTG: 1588 occurrences, specificity: 0.907\n",
            "   ‚Ä¢ TTGTTT: 1571 occurrences, specificity: 0.911\n",
            "   ‚Ä¢ ATTTAT: 1552 occurrences, specificity: 0.921\n",
            "   ‚Ä¢ TATTTA: 1537 occurrences, specificity: 0.922\n",
            "   ‚Ä¢ TTTATT: 1489 occurrences, specificity: 0.924\n",
            "   ‚Ä¢ GTTTGT: 1429 occurrences, specificity: 0.904\n",
            "   ‚Ä¢ AAAAAA: 1233 occurrences, specificity: 0.946\n",
            "   ‚Ä¢ TGCTTG: 1151 occurrences, specificity: 0.883\n",
            "   ‚Ä¢ TGATTG: 1127 occurrences, specificity: 0.900\n",
            "   ‚Ä¢ GCTTGC: 1126 occurrences, specificity: 0.890\n",
            "   ‚Ä¢ ATTGAT: 1093 occurrences, specificity: 0.901\n",
            "   ‚Ä¢ TTGATT: 1088 occurrences, specificity: 0.904\n",
            "üìä Correct predictions: confidence=0.377, uncertainty=1.096\n",
            "üìä Incorrect predictions: confidence=0.358, uncertainty=1.088\n",
            "\n",
            "üíä TUFAB Predictions:\n",
            "----------------------------------------\n",
            "‚úÖ VALIDATED k-mers (specificity > 0.8):\n",
            "   ‚Ä¢ AAAAAA: 2195 occurrences, specificity: 0.894\n",
            "   ‚Ä¢ ACAAAC: 1388 occurrences, specificity: 0.859\n",
            "   ‚Ä¢ AACAAA: 1359 occurrences, specificity: 0.861\n",
            "   ‚Ä¢ CAAACA: 1336 occurrences, specificity: 0.860\n",
            "   ‚Ä¢ AAACAA: 1306 occurrences, specificity: 0.855\n",
            "   ‚Ä¢ ACTGAC: 1275 occurrences, specificity: 0.900\n",
            "   ‚Ä¢ TGACTG: 1254 occurrences, specificity: 0.897\n",
            "   ‚Ä¢ ACGCAC: 1215 occurrences, specificity: 0.916\n",
            "   ‚Ä¢ AAGAAA: 1207 occurrences, specificity: 0.889\n",
            "   ‚Ä¢ ACGTAC: 1202 occurrences, specificity: 0.896\n",
            "   ‚Ä¢ GACTGA: 1202 occurrences, specificity: 0.904\n",
            "   ‚Ä¢ CGCACG: 1190 occurrences, specificity: 0.928\n",
            "   ‚Ä¢ AAAGAA: 1168 occurrences, specificity: 0.885\n",
            "   ‚Ä¢ GTTCGT: 1135 occurrences, specificity: 0.933\n",
            "   ‚Ä¢ AGAAAG: 1130 occurrences, specificity: 0.879\n",
            "üìä Correct predictions: confidence=0.363, uncertainty=1.105\n",
            "üìä Incorrect predictions: confidence=0.354, uncertainty=1.109\n",
            "\n",
            "üíä OQXA Predictions:\n",
            "----------------------------------------\n",
            "‚úÖ VALIDATED k-mers (specificity > 0.8):\n",
            "   ‚Ä¢ GCGCGC: 2663 occurrences, specificity: 0.909\n",
            "   ‚Ä¢ CGCGCG: 2456 occurrences, specificity: 0.908\n",
            "   ‚Ä¢ GCAGGC: 1575 occurrences, specificity: 0.909\n",
            "   ‚Ä¢ AAAAAA: 1416 occurrences, specificity: 0.908\n",
            "   ‚Ä¢ CAGGCA: 1353 occurrences, specificity: 0.910\n",
            "   ‚Ä¢ AGGCAG: 1346 occurrences, specificity: 0.909\n",
            "   ‚Ä¢ GGCAGG: 1239 occurrences, specificity: 0.908\n",
            "   ‚Ä¢ CGCCCG: 1162 occurrences, specificity: 0.906\n",
            "   ‚Ä¢ GCCAGC: 1128 occurrences, specificity: 0.908\n",
            "   ‚Ä¢ AGCCAG: 1069 occurrences, specificity: 0.908\n",
            "   ‚Ä¢ CGCTCG: 1061 occurrences, specificity: 0.906\n",
            "   ‚Ä¢ GCCCGC: 1044 occurrences, specificity: 0.906\n",
            "   ‚Ä¢ CAGCCA: 1017 occurrences, specificity: 0.909\n",
            "   ‚Ä¢ CGGTCG: 996 occurrences, specificity: 0.907\n",
            "   ‚Ä¢ GCCGGC: 982 occurrences, specificity: 0.906\n",
            "üìä Correct predictions: confidence=0.377, uncertainty=1.091\n",
            "üìä Incorrect predictions: confidence=0.349, uncertainty=1.102\n",
            "\n",
            "üîç Top k-mers to validate: ['AACAAA', 'CTGGCT', 'GGTGGG', 'AAAAAA', 'TTTCTT', 'GCCGGC', 'ATTTAT', 'ACAAAC', 'TTATTT', 'CCGGCC', 'TGGATG', 'TGGGTG', 'ATATAT', 'TGATTG', 'TTTGTT']\n",
            "\n",
            "üß™ VALIDATING K-MER IMPORTANCE WITH ABLATION\n",
            "============================================================\n",
            "üìä ABLATION RESULTS:\n",
            "   ‚Ä¢ Original accuracy: 0.863\n",
            "   ‚Ä¢ Masked accuracy: 0.653\n",
            "   ‚Ä¢ Performance drop: 0.209 (24.3%)\n",
            "‚úÖ VALID: K-mers are actually important (significant performance drop)\n",
            "\n",
            "üìä STATISTICAL VALIDATION OF K-MER PATTERNS\n",
            "==================================================\n",
            "\n",
            "======================================================================\n",
            "üéØ BAYESIAN UNCERTAINTY & K-MER PATTERNS\n",
            "======================================================================\n",
            "üìä Uncertainty Analysis:\n",
            "   ‚Ä¢ High uncertainty predictions: 300\n",
            "   ‚Ä¢ Low uncertainty predictions: 300\n",
            "\n",
            "üîç K-mers in HIGH uncertainty predictions:\n",
            "   ‚Ä¢ AAAAAA: 6854 occurrences (vs 5443 in low uncertainty)\n",
            "   ‚Ä¢ TTTTTT: 4320 occurrences (vs 3573 in low uncertainty)\n",
            "   ‚Ä¢ AAGAAA: 3881 occurrences (vs 3579 in low uncertainty)\n",
            "   ‚Ä¢ AAAGAA: 3814 occurrences (vs 3580 in low uncertainty)\n",
            "   ‚Ä¢ CGGCCG: 3584 occurrences (vs 2196 in low uncertainty)\n",
            "\n",
            "üîç K-mers in LOW uncertainty predictions:\n",
            "   ‚Ä¢ AAAAAA: 5443 occurrences (vs 6854 in high uncertainty)\n",
            "   ‚Ä¢ AAAGAA: 3580 occurrences (vs 3814 in high uncertainty)\n",
            "   ‚Ä¢ AAGAAA: 3579 occurrences (vs 3881 in high uncertainty)\n",
            "   ‚Ä¢ TTTTTT: 3573 occurrences (vs 4320 in high uncertainty)\n",
            "   ‚Ä¢ GAAAGA: 3533 occurrences (vs 3504 in high uncertainty)\n",
            "\n",
            "======================================================================\n",
            "üìã BAYESIAN METAOPTNET INTERPRETATION REPORT\n",
            "======================================================================\n",
            "üìà Overall Test Accuracy: 86.9%\n",
            "   ‚Ä¢ Correct predictions: 1303\n",
            "   ‚Ä¢ Incorrect predictions: 197\n",
            "\n",
            "‚ö†Ô∏è  MODEL STRENGTHS (UNVALIDATED - interpret with caution):\n",
            "\n",
            "üîç MODEL CONFUSIONS:\n",
            "   ‚Ä¢ ERM ‚Üí VIM: 17 misclassifications\n",
            "   ‚Ä¢ APH3-DPRIME ‚Üí ERM: 10 misclassifications\n",
            "   ‚Ä¢ VIM ‚Üí ERM: 9 misclassifications\n",
            "   ‚Ä¢ ERM ‚Üí LRA: 8 misclassifications\n",
            "   ‚Ä¢ CATB ‚Üí QNRB: 7 misclassifications\n",
            "   ‚Ä¢ APH3-DPRIME ‚Üí CATB: 7 misclassifications\n",
            "   ‚Ä¢ APH3-DPRIME ‚Üí VIM: 6 misclassifications\n",
            "   ‚Ä¢ MCR ‚Üí PARC: 6 misclassifications\n",
            "   ‚Ä¢ QNRB ‚Üí QNRS: 6 misclassifications\n",
            "   ‚Ä¢ ERM ‚Üí QNRS: 6 misclassifications\n",
            "\n",
            "üîë KEY INSIGHTS:\n",
            "   1. Your Bayesian MetaOptNet achieves 86.9% accuracy\n",
            "   2. ‚ö†Ô∏è  K-mer analysis UNVALIDATED - interpret patterns cautiously\n",
            "   3. High-uncertainty predictions often have unusual k-mer patterns\n",
            "   4. Model confidence correlates with presence of characteristic k-mers\n",
            "\n",
            "======================================================================\n",
            "‚úÖ VALIDATED ANALYSIS COMPLETE!\n",
            "======================================================================\n",
            "‚ö†Ô∏è  Interpret results with caution:\n",
            "   ‚Ä¢ K-mers may be correlative rather than causal\n",
            "   ‚Ä¢ Consider additional validation methods\n",
            "\n",
            "You now understand:\n",
            "   ‚Ä¢ What DNA patterns your Bayesian MetaOptNet uses for decisions\n",
            "   ‚Ä¢ Which classes it understands well vs confuses\n",
            "   ‚Ä¢ How Bayesian uncertainty relates to DNA patterns\n",
            "   ‚Ä¢ Whether k-mer patterns are statistically validated\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}